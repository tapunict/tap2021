{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Spark Mlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://blog.osservatori.net/hubfs/AI/machine-learning.jpg)\n",
    "[Osservatori.net](https://blog.osservatori.net/it_it/machine-learning-come-funziona-apprendimento-automatico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# @reboot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.26.223.142:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TapDataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4ce84f0990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "findspark.find( ) \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TapDataFrame\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"jumbotron\">\n",
    "    <center>\n",
    "        <b>MLlib</b> is Apache Spark's scalable machine learning library.\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ease of Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***Usable in Java, Scala, Python, and R.***\n",
    "\n",
    "MLlib fits into Spark's APIs and interoperates with NumPy in Python (as of Spark 0.9) and R libraries (as of Spark 1.5). You can use any Hadoop data source (e.g. HDFS, HBase, or local files), making it easy to plug into Hadoop workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "  .load(\"hdfs://...\")\n",
    "\n",
    "model = KMeans(k=10).fit(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***High-quality algorithms, 100x faster than MapReduce.***\n",
    "\n",
    "Spark excels at iterative computation, enabling MLlib to run fast. At the same time, we care about algorithmic performance: MLlib contains high-quality algorithms that leverage iteration, and can yield better results than the one-pass approximations sometimes used on MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://spark.apache.org/images/logistic-regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algorithms and Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Algorithms*\n",
    "\n",
    "* Classification: logistic regression, naive Bayes,...\n",
    "* Regression: generalized linear regression, survival regression,...\n",
    "* Decision trees, random forests, and gradient-boosted trees\n",
    "* Recommendation: alternating least squares (ALS)\n",
    "* Clustering: K-means, Gaussian mixtures (GMMs),...\n",
    "* Topic modeling: latent Dirichlet allocation (LDA)\n",
    "* Frequent itemsets, association rules, and sequential pattern mining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Utilities*\n",
    "\n",
    "* Feature transformations: standardization, normalization, hashing,...\n",
    "* ML Pipeline construction\n",
    "* Model evaluation and hyper-parameter tuning\n",
    "* ML persistence: saving and loading models and Pipelines\n",
    "* Distributed linear algebra: SVD, PCA,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Announcement: DataFrame-based API is primary API\n",
    "https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "The MLlib RDD-based API is now in maintenance mode.\n",
    "\n",
    "As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.\n",
    "\n",
    "DataFrames provide a more user-friendly API than RDDs. The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\n",
    "The DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\n",
    "DataFrames facilitate practical ML Pipelines, particularly feature transformations. See the Pipelines guide for details.\n",
    "\n",
    "What is “Spark ML”?\n",
    "\n",
    "“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Highlights in 3.0[](https://spark.apache.org/docs/latest/ml-guide.html#highlights-in-30)\n",
    "\n",
    "The list below highlights some of the new features and enhancements added to MLlib in the `3.0`\n",
    "release of Spark:\n",
    "\n",
    "* Multiple columns support was added to `Binarizer` ([SPARK-23578](https://issues.apache.org/jira/browse/SPARK-23578)), `StringIndexer` ([SPARK-11215](https://issues.apache.org/jira/browse/SPARK-11215)), `StopWordsRemover` ([SPARK-29808](https://issues.apache.org/jira/browse/SPARK-29808)) and PySpark `QuantileDiscretizer` ([SPARK-22796](https://issues.apache.org/jira/browse/SPARK-22796)).\n",
    "* Tree-Based Feature Transformation was added\n",
    "    ([SPARK-13677](https://issues.apache.org/jira/browse/SPARK-13677)).\n",
    "* Two new evaluators `MultilabelClassificationEvaluator` ([SPARK-16692](https://issues.apache.org/jira/browse/SPARK-16692)) and `RankingEvaluator` ([SPARK-28045](https://issues.apache.org/jira/browse/SPARK-28045)) were added.\n",
    "* Sample weights support was added in `DecisionTreeClassifier/Regressor` ([SPARK-19591](https://issues.apache.org/jira/browse/SPARK-19591)), `RandomForestClassifier/Regressor` ([SPARK-9478](https://issues.apache.org/jira/browse/SPARK-9478)), `GBTClassifier/Regressor` ([SPARK-9612](https://issues.apache.org/jira/browse/SPARK-9612)),  `MulticlassClassificationEvaluator` ([SPARK-24101](https://issues.apache.org/jira/browse/SPARK-24101)), `RegressionEvaluator` ([SPARK-24102](https://issues.apache.org/jira/browse/SPARK-24102)), `BinaryClassificationEvaluator` ([SPARK-24103](https://issues.apache.org/jira/browse/SPARK-24103)), `BisectingKMeans` ([SPARK-30351](https://issues.apache.org/jira/browse/SPARK-30351)), `KMeans` ([SPARK-29967](https://issues.apache.org/jira/browse/SPARK-29967)) and `GaussianMixture` ([SPARK-30102](https://issues.apache.org/jira/browse/SPARK-30102)).\n",
    "* R API for `PowerIterationClustering` was added\n",
    "    ([SPARK-19827](https://issues.apache.org/jira/browse/SPARK-19827)).\n",
    "* Added Spark ML listener for tracking ML pipeline status\n",
    "    ([SPARK-23674](https://issues.apache.org/jira/browse/SPARK-23674)).\n",
    "* Fit with validation set was added to Gradient Boosted Trees in Python\n",
    "    ([SPARK-24333](https://issues.apache.org/jira/browse/SPARK-24333)).\n",
    "* [`RobustScaler`](https://spark.apache.org/docs/latest/ml-features.html#robustscaler) transformer was added\n",
    "    ([SPARK-28399](https://issues.apache.org/jira/browse/SPARK-28399)).\n",
    "* [`Factorization Machines`](https://spark.apache.org/docs/latest/ml-classification-regression.html#factorization-machines) classifier and regressor were added\n",
    "    ([SPARK-29224](https://issues.apache.org/jira/browse/SPARK-29224)).\n",
    "* Gaussian Naive Bayes Classifier ([SPARK-16872](https://issues.apache.org/jira/browse/SPARK-16872)) and Complement Naive Bayes Classifier ([SPARK-29942](https://issues.apache.org/jira/browse/SPARK-29942)) were added.\n",
    "* ML function parity between Scala and Python\n",
    "    ([SPARK-28958](https://issues.apache.org/jira/browse/SPARK-28958)).\n",
    "* `predictRaw` is made public in all the Classification models. `predictProbability` is made public in all the Classification models except `LinearSVCModel`\n",
    "    ([SPARK-30358](https://issues.apache.org/jira/browse/SPARK-30358))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Local vector\n",
    "A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. \n",
    "\n",
    "MLlib supports two types of local vectors: dense and sparse. \n",
    "\n",
    "A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. \n",
    "\n",
    "For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Correlation* computes the correlation matrix for the input Dataset of Vectors using the specified method. \n",
    "The output will be a DataFrame that contains the correlation matrix of the column of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         features|\n",
      "+-----------------+\n",
      "|[1.0,2.0,3.0,4.0]|\n",
      "|[2.0,4.0,6.0,8.0]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "datasetA = [1.0,2.0,3.0,4.0]\n",
    "datasetB = [2.0,4.0,6.0,8.0]\n",
    "\n",
    "data = [\n",
    "        (Vectors.dense(datasetA),),\n",
    "        (Vectors.dense(datasetB),)\n",
    "       ]\n",
    "# the comma after the variable forces Python to consider it as a tuple\n",
    "# https://www.w3schools.com/python/gloss_python_tuple_one_item.asp\n",
    "# Credits Ernesto Casablanca, TAP 2021-04-26\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1., 1., 1., 1.],\n",
      "             [1., 1., 1., 1.],\n",
      "             [1., 1., 1., 1.],\n",
      "             [1., 1., 1., 1.]])\n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1., 1., 1., 1.],\n",
      "             [1., 1., 1., 1.],\n",
      "             [1., 1., 1., 1.],\n",
      "             [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hypothesis testing\n",
    "Hypothesis testing is a powerful tool in statistics to determine whether a result is statistically significant, whether this result occurred by chance or not. spark.ml currently supports Pearson’s Chi-squared ( χ2) tests for independence.\n",
    "\n",
    "ChiSquareTest conducts Pearson’s independence test for every feature against the label. For each feature, the (feature, label) pairs are converted into a contingency matrix for which the Chi-squared statistic is computed. All label and feature values must be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|label|     features|\n",
      "+-----+-------------+\n",
      "|    1|[333.0,388.0]|\n",
      "|    2|[333.0,322.0]|\n",
      "|    3|[333.0,314.0]|\n",
      "|    4|[333.0,316.0]|\n",
      "|    5|[333.0,344.0]|\n",
      "|    6|[333.0,316.0]|\n",
      "+-----+-------------+\n",
      "\n",
      "pValues: [1.0,0.24239216167051258]\n",
      "degreesOfFreedom: [0, 20]\n",
      "statistics: [0.0,24.00000000000001]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "#https://it.wikipedia.org/wiki/Test_chi_quadrato\n",
    "data = [(1, Vectors.dense(333,388)),\n",
    "        (2, Vectors.dense(333,322)),\n",
    "        (3, Vectors.dense(333,314)),\n",
    "        (4, Vectors.dense(333,316)),\n",
    "        (5, Vectors.dense(333,344)),\n",
    "        (6, Vectors.dense(333,316))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "df.show()\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summarizer\n",
    "We provide vector column summary statistics for Dataframe through Summarizer.\n",
    "\n",
    "Available metrics are the column-wise max, min, mean, variance, and number of nonzeros, as well as the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.stat.SummaryBuilder at 0x7f4cd86484d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Usage of spark.sparkContext to get sc \n",
    "\n",
    "df = spark.sparkContext.parallelize(\n",
    "    [Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]\n",
    "    ).toDF()\n",
    "\n",
    "# create summarizer for multiple metrics \"mean\" and \"count\"\n",
    "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
    "\n",
    "summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|aggregate_metrics(features, weight)|\n",
      "+-----------------------------------+\n",
      "|{[1.0,1.0,1.0], 1}                 |\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute statistics for multiple metrics with weight\n",
    "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|aggregate_metrics(features, 1.0)|\n",
      "+--------------------------------+\n",
      "|{[1.0,1.5,2.0], 2}              |\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute statistics for multiple metrics without weight\n",
    "df.select(summarizer.summary(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.0,1.0] |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute statistics for single metric \"mean\" with weight\n",
    "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.5,2.0] |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute statistics for single metric \"mean\" without weight\n",
    "df.select(Summarizer.mean(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow. \n",
    "\n",
    "This section covers the key concepts introduced by the Pipelines API, where the pipeline concept is mostly inspired by the scikit-learn project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**DataFrame**\n",
    "\n",
    "This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. \n",
    "\n",
    "E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Transformer** \n",
    "\n",
    "A Transformer is an algorithm which can transform one DataFrame into another DataFrame.\n",
    "\n",
    "E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Estimator**\n",
    "\n",
    "An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. \n",
    "\n",
    "E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pipeline**\n",
    "\n",
    "A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Parameter** \n",
    "\n",
    "All Transformers and Estimators now share a common API for specifying parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame\n",
    "Machine learning can be applied to a wide variety of data types, such as vectors, text, images, and structured data. \n",
    "\n",
    "This API adopts the DataFrame from Spark SQL in order to support a variety of data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DataFrame supports many basic and structured types; see the Spark SQL datatype reference for a list of supported types. In addition to the types listed in the Spark SQL guide, DataFrame can use ML Vector types.\n",
    "\n",
    "A DataFrame can be created either implicitly or explicitly from a regular RDD. See the code examples below and the Spark SQL programming guide for examples.\n",
    "\n",
    "Columns in a DataFrame are named. The code examples below use names such as “text,” “features,” and “label.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformers\n",
    "A Transformer is an abstraction that includes feature transformers and learned models. \n",
    "\n",
    "Technically, a Transformer implements a method transform(), which converts one DataFrame into another, generally by appending one or more columns. \n",
    "\n",
    "For example:\n",
    "* A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.\n",
    "* A learning model might take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimators\n",
    "An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. \n",
    "\n",
    "Technically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer. \n",
    "\n",
    "For example, a learning algorithm such as LogisticRegression is an Estimator, \n",
    "and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pipeline\n",
    "In machine learning, it is common to run a sequence of algorithms to process and learn from data. E.g., a simple text document processing workflow might include several stages:\n",
    "\n",
    "* Split each document’s text into words.\n",
    "* Convert each document’s words into a numerical feature vector.\n",
    "* Learn a prediction model using the feature vectors and labels.\n",
    "\n",
    "MLlib represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How it works\n",
    "A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. \n",
    "\n",
    "These stages are run in order, and the input DataFrame is transformed as it passes through each stage. \n",
    "\n",
    "For Transformer stages, the transform() method is called on the DataFrame. \n",
    "\n",
    "For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline), and that Transformer’s transform() method is called on the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A simple Text document workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://spark.apache.org/docs/latest/img/ml-Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Above, the top row represents a Pipeline with three stages. The first two (Tokenizer and HashingTF) are Transformers (blue), and the third (LogisticRegression) is an Estimator (red). The bottom row represents data flowing through the pipeline, where cylinders indicate DataFrames. The Pipeline.fit() method is called on the original DataFrame, which has raw text documents and labels. \n",
    "\n",
    "The Tokenizer.transform() method splits the raw text documents into words, adding a new column with words to the DataFrame. \n",
    "\n",
    "The HashingTF.transform() method converts the words column into feature vectors, adding a new column with those vectors to the DataFrame. Now, since LogisticRegression is an Estimator, the Pipeline first calls LogisticRegression.fit() to produce a LogisticRegressionModel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://spark.apache.org/docs/latest/img/ml-PipelineModel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the figure above, the PipelineModel has the same number of stages as the original Pipeline, but all Estimators in the original Pipeline have become Transformers. When the PipelineModel’s transform() method is called on a test dataset, the data are passed through the fitted pipeline in order. Each stage’s transform() method updates the dataset and passes it to the next stage.\n",
    "\n",
    "Pipelines and PipelineModels help to ensure that training and test data go through identical feature processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ML persistence: Saving and Loading Pipelines\n",
    "Often times it is worth it to save a model or a pipeline to disk for later use. In Spark 1.6, a model import/export functionality was added to the Pipeline API. As of Spark 2.3, the DataFrame-based API in spark.ml and pyspark.ml has complete coverage.\n",
    "\n",
    "ML persistence works across Scala, Java and Python. However, R currently uses a modified format, so models saved in R can only be loaded back in R;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_bfc473e6780c"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_bfc473e6780c"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|          text|\n",
      "+---+--------------+\n",
      "|  4|   spark i j k|\n",
      "|  5|        batman|\n",
      "|  6|spark a hadoop|\n",
      "|  7| apache hadoop|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"batman\"),\n",
    "    (6, \"spark a hadoop\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|          text|             words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+--------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|   spark i j k|  [spark, i, j, k]|(262144,[19036,68...|[-1.6609033227473...|[0.15964077387874...|       1.0|\n",
      "|  5|        batman|          [batman]|(262144,[178334],...|[1.64218895265635...|[0.83783256854766...|       0.0|\n",
      "|  6|spark a hadoop|[spark, a, hadoop]|(262144,[107107,1...|[-2.0753881790526...|[0.11151207471547...|       1.0|\n",
      "|  7| apache hadoop|  [apache, hadoop]|(262144,[68303,19...|[4.00817033336806...|[0.98215753334442...|       0.0|\n",
      "+---+--------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.1596407738787412,0.8403592261212588], prediction=1.000000\n",
      "(5, batman) --> prob=[0.8378325685476614,0.16216743145233858], prediction=0.000000\n",
      "(6, spark a hadoop) --> prob=[0.11151207471547728,0.8884879252845227], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9821575333444208,0.017842466655579203], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, a b c d e spark) --> prob=[0.0021342419881406768,0.9978657580118593], prediction=1.000000\n",
      "(1, b d) --> prob=[0.9959176174854043,0.004082382514595695], prediction=0.000000\n",
      "(2, spark f g h) --> prob=[0.0014541569986711246,0.9985458430013289], prediction=1.000000\n",
      "(3, hadoop mapreduce) --> prob=[0.9982978367343561,0.0017021632656438745], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on training documents and print columns of interest.\n",
    "prediction = model.transform(training)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_896363d5c6be,\n",
       " HashingTF_03b04dbc6b7d,\n",
       " LogisticRegressionModel: uid=LogisticRegression_df4027938854, numClasses=2, numFeatures=262144]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract \n",
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "modelSummary = model.stages[2].summary\n",
    "\n",
    "modelSummary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.classification.BinaryLogisticRegressionTrainingSummary at 0x7f4ce8291cd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, mapreduce hadopp spark) --> prob=[0.6693126798261014,0.33068732017389857], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "test2 = spark.createDataFrame([\n",
    "    (8, \"mapreduce hadopp spark\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on training documents and print columns of interest.\n",
    "prediction = model.transform(test2)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extracting, transforming and selecting features\n",
    "https://spark.apache.org/docs/latest/ml-features.html#extracting-transforming-and-selecting-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Extraction: Extracting features from “raw” data\n",
    "* Transformation: Scaling, converting, or modifying features\n",
    "* Selection: Selecting a subset from a larger set of features\n",
    "* Locality Sensitive Hashing (LSH): This class of algorithms combines aspects of feature transformation with other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word2Vec\n",
    "Word2Vec computes distributed vector representation of words. \n",
    "\n",
    "The main advantage of the distributed representations is that similar words are close in the vector space, which makes generalization to novel patterns easier and model estimation more robust. \n",
    "\n",
    "Distributed vector representation is showed to be useful in many natural language processing applications such as named entity recognition, disambiguation, parsing, tagging and machine translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|text                                      |\n",
      "+------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |\n",
      "|[I, wish, Java, could, use, case, classes]|\n",
      "|[Logistic, regression, models, are, neat] |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    " \n",
    "documentDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2VecModel: uid=Word2Vec_7b2f7e4a7bea, numWords=16, vectorSize=3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.038781297206878666,-0.0880258545279503,-0.015223285555839539]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.013235008610146386,-0.014181817748716899,-0.018456950783729553]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.02225852077826858,-0.0025158967822790147,-0.028219491243362427]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification and regression\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#classification-and-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Logistic regression\n",
    "Logistic regression is a popular method to predict a categorical response. It is a special case of Generalized Linear models that predicts the probability of the outcomes. In spark.ml logistic regression can be used to predict a binary outcome by using binomial logistic regression, or it can be used to predict a multiclass outcome by using multinomial logistic regression. Use the family parameter to select between these two algorithms, or leave it unset and Spark will infer the correct variant.\n",
    "\n",
    "> Multinomial logistic regression can be used for binary classification by setting the family param to “multinomial”. It will produce two sets of coefficients and two intercepts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(\"../spark/dataset/sample_libsvm_data.txt\")\n",
    "#training.show(truncate=False)\n",
    "training.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.353983524188241e-05,-9.102738505589566e-05,-0.0001946743054690423,-0.00020300642473486603,-3.147618331486458e-05,-6.842977602660821e-05,1.5883626898236275e-05,1.4023497091368928e-05,0.0003543204752496838,0.00011443272898171099,0.00010016712383666487,0.0006014109303795511,0.0002840248179122765,-0.00011541084736508905,0.000385996886312906,0.0006350195574241097,-0.00011506412384575733,-0.0001527186586498689,0.0002804933808994214,0.0006070117471191665,-0.0002008459663247435,-0.00014210755792901347,0.0002739010341160883,0.0002773045624496811,-9.838027027269408e-05,-0.00038085224435175833,-0.00025315198008554285,0.0002774771477075434,-0.00024436197639191286,-0.0015394744687597679,-0.00023073328411330604])\n",
      "Intercept: 0.22456315961250245\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial coefficients: 2 X 692 CSRMatrix\n",
      "(0,244) 0.0\n",
      "(0,263) 0.0001\n",
      "(0,272) 0.0001\n",
      "(0,300) 0.0001\n",
      "(0,350) -0.0\n",
      "(0,351) -0.0\n",
      "(0,378) -0.0\n",
      "(0,379) -0.0\n",
      "(0,405) -0.0\n",
      "(0,406) -0.0006\n",
      "(0,407) -0.0001\n",
      "(0,428) 0.0001\n",
      "(0,433) -0.0\n",
      "(0,434) -0.0007\n",
      "(0,455) 0.0001\n",
      "(0,456) 0.0001\n",
      "..\n",
      "..\n",
      "Multinomial intercepts: [-0.12065879445860596,0.12065879445860596]\n"
     ]
    }
   ],
   "source": [
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    " \n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.6833149135741672\n",
      "0.6662875751473734\n",
      "0.6217068546034616\n",
      "0.6127265245887888\n",
      "0.6060347986802872\n",
      "0.6031750687571563\n",
      "0.5969621534836272\n",
      "0.594074303198312\n",
      "0.5906089243339021\n",
      "0.5894724576491043\n",
      "0.5882187775729588\n"
     ]
    }
   ],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.017543859649122806|\n",
      "|0.0| 0.03508771929824561|\n",
      "|0.0| 0.05263157894736842|\n",
      "|0.0| 0.07017543859649122|\n",
      "|0.0| 0.08771929824561403|\n",
      "|0.0| 0.10526315789473684|\n",
      "|0.0| 0.12280701754385964|\n",
      "|0.0| 0.14035087719298245|\n",
      "|0.0| 0.15789473684210525|\n",
      "|0.0| 0.17543859649122806|\n",
      "|0.0| 0.19298245614035087|\n",
      "|0.0| 0.21052631578947367|\n",
      "|0.0| 0.22807017543859648|\n",
      "|0.0| 0.24561403508771928|\n",
      "|0.0|  0.2631578947368421|\n",
      "|0.0|  0.2807017543859649|\n",
      "|0.0|  0.2982456140350877|\n",
      "|0.0|  0.3157894736842105|\n",
      "|0.0|  0.3333333333333333|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_7c10b85de11b"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering\n",
    "https://spark.apache.org/docs/latest/ml-clustering.html#clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "K-means\n",
    "k-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. The MLlib implementation includes a parallelized variant of the k-means++ method called kmeans||.\n",
    "\n",
    "KMeans is implemented as an Estimator and generates a KMeansModel as the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Input Columns**\n",
    "\n",
    "|Param name\t|  Type(s)\t| Default\t| Description  | \n",
    "|-----------|  ------   | ----------| ----------   |\n",
    "|featuresCol|  Vector\t|\"features\"\t|Feature vector|\n",
    "\n",
    "**Output Columns**\n",
    "\n",
    "|Param name\t|  Type(s)\t| Default\t| Description  | \n",
    "|-----------|  ------   | ----------| ----------   |\n",
    "|predictionCol|\tInt|\t\"prediction\"|\tPredicted cluster center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"../spark/dataset/sample_kmeans_data.txt\")\n",
    "#dataset.show(truncate=False)\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|            features|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|           (3,[],[])|         1|\n",
      "|  1.0|(3,[0,1,2],[0.1,0...|         1|\n",
      "|  2.0|(3,[0,1,2],[0.2,0...|         1|\n",
      "|  3.0|(3,[0,1,2],[9.0,9...|         0|\n",
      "|  4.0|(3,[0,1,2],[9.1,9...|         0|\n",
      "|  5.0|(3,[0,1,2],[9.2,9...|         0|\n",
      "+-----+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[9.1 9.1 9.1]\n",
      "[0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Collaborative Filtering \n",
    "https://spark.apache.org/docs/latest/ml-collaborative-filtering.html#collaborative-filtering-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example on demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://media1.tenor.com/images/257a13ee5e204efdca4bb135a8f75a2e/tenor.gif?itemid=16088629)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Biblio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* https://spark.apache.org/mllib/\n",
    "* https://spark.apache.org/docs/latest/ml-guide.html\n",
    "* https://blog.osservatori.net/it_it/machine-learning-come-funziona-apprendimento-automatico\n",
    "* https://towardsdatascience.com/hands-on-big-data-streaming-apache-spark-at-scale-fd89c15fa6b0\n",
    "* https://towardsdatascience.com/apache-spark-mllib-tutorial-ec6f1cb336a9\n",
    "* https://www.guru99.com/pyspark-tutorial.html\n",
    "* https://towardsdatascience.com/sentiment-analysis-simplified-ac30720a5827\n",
    "* http://web.cs.ucla.edu/~mtgarip/statistics.html\n",
    "* https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n",
    "* https://runawayhorse001.github.io/LearningApacheSpark/index.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "enable_chalkboard": "true",
   "scroll": "true",
   "theme": "simple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}